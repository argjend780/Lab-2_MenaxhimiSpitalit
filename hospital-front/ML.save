# file: model_versioning.py
import os
import json
import joblib
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold, cross_val_score
from scipy.sparse import hstack, csr_matrix
import numpy as np
import pandas as pd

# Krijo folderin nese nuk ekziston
os.makedirs("ml_model", exist_ok=True)

# Trajnimi i modelit
train_data = pd.DataFrame({
    "notes": [
        "Well-managed type 2 diabetes with normal glucose",
        "Mild diabetes with occasional high glucose",
        "Diabetes with controlled glucose but high cholesterol",
        "Type 1 diabetes with stable glucose levels",
        "Type 2 diabetes with retinopathy and high blood pressure",
        "Type 1 diabetes with moderate glucose fluctuations",
        "Mild diabetes with normal glucose and no complications",
        "Severe insulin resistance and kidney issues",
        "Diabetic ketoacidosis and elevated ketones",
        "Uncontrolled type 2 diabetes with high HbA1c",
        "Type 1 diabetes with frequent hypoglycemia",
        "Mild diabetes with normal glucose and no symptoms",
        "Well-managed diabetes with good diet and exercise"
    ],
    "lab_values": [
        [7.2, 7.0, 7.1],
        [7.8, 8.0, 7.9],
        [8.5, 8.7, 8.6],
        [6.8, 6.9, 6.7],
        [9.5, 9.7, 9.6],
        [10.5, 10.7, 10.3],
        [8.0, 8.1, 8.0],
        [14.0, 14.2, 13.8],
        [11.0, 11.2, 10.8],
        [13.5, 13.7, 13.6],
        [16.0, 16.2, 15.8],
        [6.9, 7.0, 6.8],
        [6.5, 6.6, 6.4]
    ],
    "risk": ["Low", "Medium", "Medium", "Low", "Medium", "Medium",
             "Low", "High", "High", "High", "High", "Low", "Low"]
})

train_data["mean_val"] = train_data["lab_values"].apply(lambda x: np.mean(x) if x else 0)
train_data["std_val"] = train_data["lab_values"].apply(lambda x: np.std(x) if x else 0)
train_data["num_vals"] = train_data["lab_values"].apply(lambda x: len(x) if x else 0)
train_data["risk_lbl"] = train_data["risk"].map({"Low": 0, "Medium": 1, "High": 2})

vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', max_features=1000, ngram_range=(1,2))
X_text = vectorizer.fit_transform(train_data["notes"])
X_num = csr_matrix(train_data[["mean_val", "std_val", "num_vals"]].values)
X = hstack([X_text, X_num])
y = train_data["risk_lbl"]

model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
model.fit(X, y)

# Kros-validim
skf = StratifiedKFold(n_splits=3)
scores = cross_val_score(model, X, y, cv=skf)
print(f"Stratified CV Accuracy: {scores.mean():.2f} ± {scores.std():.2f}")

# Versionimi dinamik
metadata_path = "ml_model/metadata.json"
if os.path.exists(metadata_path):
    with open(metadata_path) as f:
        metadata = json.load(f)
else:
    metadata = {"current_model": "", "current_vectorizer": "", "history": []}

existing_versions = [entry["version"] for entry in metadata["history"]]
latest_version = max([int(v[1:]) for v in existing_versions], default=0)
new_version = f"v{latest_version + 1}"

model_name = f"model_{new_version}.pkl"
vectorizer_name = f"vectorizer_{new_version}.pkl"

joblib.dump(model, f"ml_model/{model_name}")
joblib.dump(vectorizer, f"ml_model/{vectorizer_name}")

metadata["current_model"] = model_name
metadata["current_vectorizer"] = vectorizer_name
metadata["history"].append({
    "version": new_version,
    "model": model_name,
    "vectorizer": vectorizer_name,
    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
})

with open(metadata_path, "w") as f:
    json.dump(metadata, f, indent=2)

print(f"Modeli {new_version} u ruajt dhe metadata u perditesua.")
# file: kafka_listener.py
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from fastapi import FastAPI
from confluent_kafka import Consumer, KafkaError
from threading import Thread
import logging
import time
import joblib
import os
import json
from google.protobuf.json_format import MessageToDict
from medical_record_pb2 import MedicalRecord
import numpy as np
from scipy.sparse import csr_matrix, hstack
from pathlib import Path

app = FastAPI()
logging.basicConfig(level=logging.INFO)

kafka_config = {
    'bootstrap.servers': 'localhost:9094',
    'group.id': 'fastapi-consumer-group',
    'auto.offset.reset': 'earliest'
}

consumer = Consumer(kafka_config)
consumer.subscribe(['medical-record'])

model = None
vectorizer = None

BASE_DIR = Path(__file__).resolve().parent


def load_model():
    global model, vectorizer
    try:
        metadata_path = BASE_DIR / "ml_model/metadata.json"
        with open(metadata_path) as f:
            meta = json.load(f)

        model_path = BASE_DIR / "ml_model" / meta["current_model"]
        vectorizer_path = BASE_DIR / "ml_model" / meta["current_vectorizer"]

        logging.info(f"Loading model from: {model_path}")
        logging.info(f"Loading vectorizer from: {vectorizer_path}")

        if not model_path.exists():
            raise FileNotFoundError(f"Model file missing at {model_path}")
        if not vectorizer_path.exists():
            raise FileNotFoundError(f"Vectorizer file missing at {vectorizer_path}")

        model = joblib.load(model_path)
        vectorizer = joblib.load(vectorizer_path)
        logging.info("ML model and vectorizer loaded successfully.")
        logging.info(f"Model version loaded: {meta['current_model']}")

    except Exception as e:
        logging.exception("Failed to load model/vectorizer")


def append_record_to_csv(record: MedicalRecord, risk: str):
    os.makedirs("kafka_data", exist_ok=True)
    path = "kafka_data/medical_records.csv"
    safe_lab_values = []

    for l in record.labResults:
        try:
            val = float(str(l.resultValue).strip('%'))
            safe_lab_values.append(val)
        except ValueError:
            continue

    entry = {
        "patient_id": record.pacientId,
        "diagnosis": record.diagnosis,
        "treatment_plan": record.treatmentPlan,
        "notes": record.notes,
        "risk": "High" if risk == "2" else "Medium" if risk == "1" else "Low",
        "lab_values": safe_lab_values,
        "created_at": record.createdAt
    }

    df = pd.DataFrame([entry])
    df.to_csv(path, mode='a', index=False, header=not os.path.exists(path))

    logging.info(f"Record appended to CSV: {entry}")


def recommend_plan(risk_level: str) -> str:
    if risk_level == "0":
        return "Rikontroll pas 6 muajsh, ndiq dieten dhe aktivitetin fizik."
    elif risk_level == "1":
        return "Konsultë me endokrinologun dhe analiza laboratorike mujore."
    elif risk_level == "2":
        return "Kontroll urgjent, konsulte mjekesore dhe monitorim te perditshem."
    return "Nuk ka rekomandim."


def predict_risk_level(record: MedicalRecord) -> tuple[str, str]:
    if model is None or vectorizer is None:
        logging.error("Model or Vectorizer is not loaded. Prediction cannot proceed.")
        return ("Error", "Model/Vectorizer not loaded")

    try:
        text_input = [record.notes] if record.notes else [""]
        X_text = vectorizer.transform(text_input)

        lab_values = []
        for lr in record.labResults:
            try:
                val = float(str(lr.resultValue).strip('%'))
                if val != 0.0:
                    lab_values.append(val)
            except (ValueError, TypeError):
                continue

        if lab_values:
            numeric = np.array([
                np.mean(lab_values),
                np.std(lab_values),
                len(lab_values)
            ]).reshape(1, -1)
        else:
            numeric = np.zeros((1, 3))

        numeric_sparse = csr_matrix(numeric)
        X_combined = hstack([X_text, numeric_sparse])
        pred = model.predict(X_combined)
        return str(pred[0]), recommend_plan(str(pred[0]))

    except Exception as e:
        logging.error("Prediction failed: %s", e)
        return ("Unknown", "Rekomandim i panjohur")


def write_to_txt(message: str):
    try:
        with open("medical_records.txt", "a", encoding="utf-8") as file:
            file.write(message + "\n" + "-"*50 + "\n")
        logging.info("Successfully wrote record to file.")
    except Exception as e:
        logging.error("Error while writing to text file: %s", e)


def process_kafka_message(message_bytes: bytes):
    try:
        medical_record = MedicalRecord()
        medical_record.ParseFromString(message_bytes)
        risk, recommendation = predict_risk_level(medical_record)
        append_record_to_csv(medical_record, risk)

        message = f"""
        Received Medical Record
        Patient ID: {medical_record.pacientId}
        Diagnosis: {medical_record.diagnosis}
        Treatment: {medical_record.treatmentPlan}
        Notes: {medical_record.notes}
        Created At: {medical_record.createdAt}
        Risk Level: {risk}
        Recommendation: {recommendation}

        Lab Results:
        """

        for lab_result in medical_record.labResults:
            message += f"""
            Test Type: {lab_result.testType}
            Result Value: {lab_result.resultValue}
            Result Date: {lab_result.resultDate}
            """

        message += "\nMedical Images:\n"
        for medical_image in medical_record.medicalImages:
            message += f"""
            Image Type: {medical_image.imageType}
            File URL: {medical_image.fileUrl}
            Uploaded At: {medical_image.uploadedAt}
            """

        write_to_txt(message)

    except Exception as e:
        logging.error("Error while processing Kafka message: %s", e)


def kafka_consume_loop():
    while True:
        try:
            msg = consumer.poll(1.0)
            if msg is None:
                continue
            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    logging.info(f"End of partition reached: {msg.partition()} {msg.offset()}")
                else:
                    logging.error(f"Kafka Error: {msg.error()}")
                continue

            process_kafka_message(msg.value())
        except Exception as e:
            logging.error("Error in Kafka consumption loop: %s", e)
        time.sleep(1)


@app.on_event("startup")
def start_kafka_listener():
    load_model()
    thread = Thread(target=kafka_consume_loop, daemon=True)
    thread.start()
    logging.info("Kafka consumer thread started")


@app.on_event("shutdown")
def shutdown():
    logging.info("Shutting down Kafka consumer...")
    consumer.close()


@app.get("/")
def health_check():
    return {"status": "Kafka listener with ML model running"}


@app.post("/recommendation")
def get_recommendation(record: dict):
    try:
        proto_record = MedicalRecord()
        proto_record.ParseFromString(record["protobuf"])
        risk, recommendation = predict_risk_level(proto_record)
        return {"risk_level": risk, "recommendation": recommendation}
    except Exception as e:
        return {"error": str(e)}